{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "# possible change to pytorch  \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path):\n",
    "    df= pd.read_csv(path)\n",
    "    return pd.DataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropDuplicatedRowAndColumn(train, test):\n",
    "    \n",
    "    train = train.drop_duplicates()\n",
    "    drop = train.columns.duplicated()\n",
    "    return [train.loc[:,~drop], test.loc[:,~drop[:len(drop)-1]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasiConstantRemoval(train, threshold, test):\n",
    "    constant_filter = VarianceThreshold(threshold= threshold)\n",
    "    constant_filter.fit(train)\n",
    "    return [pd.DataFrame(constant_filter.transform(train)),pd.DataFrame(constant_filter.transform(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropCorrelatedFeatures(train,test):\n",
    "    drop_correlated = DropCorrelatedFeatures(\n",
    "    variables=None, method='pearson', threshold=0.9)\n",
    "    drop_correlated.fit(train)\n",
    "    return [pd.DataFrame(drop_correlated.transform(train)),pd.DataFrame(drop_correlated.transform(train))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropColumnWithName(data, name):\n",
    "    column_to_drop = [c for c in data if c.startswith(name)]\n",
    "    data = data.drop(columns = column_to_drop )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(data):\n",
    "    scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(data))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df_train = loadData(\"train.csv\")\n",
    "    df_test = loadData(\"test.csv\")\n",
    "    \n",
    "    # dropping \"ID\" from both training and test data\n",
    "    ID_train = df_train[\"ID\"].copy()\n",
    "    ID_test = df_test[\"ID\"].copy()\n",
    "    df_train = df_train.drop(columns = \"ID\")\n",
    "    df_test = df_test.drop(columns = \"ID\")\n",
    "    \n",
    "\n",
    "    dup = dropDuplicatedRowAndColumn(df_train, df_test)\n",
    "    df_train = dup[0]\n",
    "    df_test = dup[1]\n",
    "    \n",
    "    y_train = df_train['TARGET'].copy()\n",
    "    df_train_x = df_train.drop(columns=\"TARGET\")\n",
    "    \n",
    "    quasi_res = quasiConstantRemoval(df_train_x, 0.01, df_test)\n",
    "    df_train_x = quasi_res[0]\n",
    "    df_test = quasi_res[1]\n",
    "    \n",
    "    correlated  = dropCorrelatedFeatures(df_train_x,df_test)\n",
    "    df_train_x = correlated[0]\n",
    "    df_test = correlated[1]\n",
    " \n",
    "    df_train_x = normalise(df_train_x)\n",
    "    print(df_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6    \\\n",
      "0      0.038772 -0.805834 -0.055166 -0.220682 -0.226444 -0.039477 -0.043503   \n",
      "1      0.038772  0.033327 -0.055166 -0.220682 -0.226444 -0.039477 -0.043503   \n",
      "2      0.038772 -0.805834 -0.055166 -0.220682 -0.226444 -0.039477 -0.043503   \n",
      "3      0.038772  0.262189 -0.055166  0.336397  0.119616 -0.039477 -0.043503   \n",
      "4      0.038772  0.414763 -0.055166 -0.220682 -0.226444 -0.039477 -0.043503   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "71208  0.038772  0.414763 -0.055166  0.853451  0.492882 -0.039477 -0.043503   \n",
      "71209  0.038772  1.101350 -0.055166 -0.220682 -0.226444 -0.039477 -0.043503   \n",
      "71210  0.038772  0.414763 -0.055166 -0.220682 -0.226444 -0.039477 -0.043503   \n",
      "71211  0.038772 -0.805834 -0.055166 -0.220682 -0.226444 -0.039477 -0.043503   \n",
      "71212  0.038772 -0.653259 -0.055166 -0.220682 -0.226444 -0.039477 -0.043503   \n",
      "\n",
      "            7         8        9    ...       151       152      153  \\\n",
      "0     -0.013941 -0.016054 -0.03428  ... -0.080022 -0.060917 -0.03566   \n",
      "1     -0.013941 -0.016054 -0.03428  ... -0.063549 -0.060917 -0.03566   \n",
      "2     -0.013941 -0.016054 -0.03428  ... -0.080022 -0.060917 -0.03566   \n",
      "3     -0.013941 -0.016054 -0.03428  ... -0.080022 -0.060917 -0.03566   \n",
      "4     -0.013941 -0.016054 -0.03428  ... -0.080022 -0.060917 -0.03566   \n",
      "...         ...       ...      ...  ...       ...       ...      ...   \n",
      "71208 -0.013941 -0.016054 -0.03428  ... -0.080022 -0.060917 -0.03566   \n",
      "71209 -0.013941 -0.016054 -0.03428  ... -0.080022 -0.060917 -0.03566   \n",
      "71210 -0.013941 -0.016054 -0.03428  ... -0.080022 -0.060917 -0.03566   \n",
      "71211 -0.013941 -0.016054 -0.03428  ... -0.080022 -0.060917 -0.03566   \n",
      "71212 -0.013941 -0.016054 -0.03428  ... -0.080022 -0.060917 -0.03566   \n",
      "\n",
      "            154       155       156       157      158       159       160  \n",
      "0     -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994 -0.414017  \n",
      "1     -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994 -0.360503  \n",
      "2     -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994 -0.264577  \n",
      "3     -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994 -0.282246  \n",
      "4     -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994  0.000940  \n",
      "...         ...       ...       ...       ...      ...       ...       ...  \n",
      "71208 -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994 -0.143019  \n",
      "71209 -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994 -0.298617  \n",
      "71210 -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994  0.007972  \n",
      "71211 -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994 -0.229011  \n",
      "71212 -0.005264 -0.003747 -0.017986 -0.012377 -0.01617 -0.012994 -0.174555  \n",
      "\n",
      "[71213 rows x 161 columns]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 fold and the measure is loss\n",
    "def build_model(data):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim = data.shape[1], output_dim=120, init='uniform', activation = 'tanh'))\n",
    "    model.add(Dense(input_dim = 120, output_dim=1, init='uniform', activation = 'sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
